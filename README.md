work-absense-forecaster
==============================

A work absense hours ML project

Setup

```
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt
```

Run MLFlow server
```
mlflow server  --backend-store-uri sqlite:///my.db   --default-artifact-root ./mlruns   --host 0.0.0.0 --port 5000
```


Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

# Testing Guide for Work Absenteeism Forecaster

## ðŸ“‹ What Has Been Created

A comprehensive test suite for the `src/models` module has been created with the following structure:

```
tests/
â”œâ”€â”€ __init__.py                    # Package initialization
â”œâ”€â”€ conftest.py                    # Shared pytest fixtures
â”œâ”€â”€ test_preprocessors.py          # Tests for custom transformers (30+ tests)
â”œâ”€â”€ test_train_model.py           # Tests for training pipeline (15+ tests)
â”œâ”€â”€ test_predict_model.py         # Tests for prediction pipeline (15+ tests)
â”œâ”€â”€ test_parameter_tuning.py      # Tests for hyperparameter tuning (10+ tests)
â”œâ”€â”€ README.md                      # Test documentation
â””â”€â”€ TEST_SUMMARY.md               # Detailed test summary

Additional files:
â”œâ”€â”€ pytest.ini                     # Pytest configuration
â”œâ”€â”€ run_tests.sh                   # Convenience script for running tests
â””â”€â”€ .github/workflows/tests.yml    # CI/CD workflow for automated testing
```

## Test Coverage

The test suite covers:

1. **Preprocessors (`test_preprocessors.py`)**
   - `DropColumnsTransformer`: Column dropping functionality
   - `IQRClippingTransformer`: Outlier handling using IQR method
   - `ToStringTransformer`: Type conversion to strings
   - Integration with sklearn pipelines

2. **Model Training (`test_train_model.py`)**
   - Pipeline construction
   - Model creation (Logistic Regression, Random Forest, Neural Network)
   - Data preparation and preprocessing
   - Training process and metrics calculation

3. **Model Prediction (`test_predict_model.py`)**
   - Model loading and saving
   - Making predictions on new data
   - Prediction evaluation with ground truth
   - Data handling in prediction pipeline

4. **Parameter Tuning (`test_parameter_tuning.py`)**
   - Parameter grid setup
   - GridSearchCV functionality
   - Best model selection
   - Metrics tracking during tuning

---

## ðŸš€ Quick Start

Build the docker image that conatins the required environment to run the tests.

```sh
docker build -f Dockerfile.test -t work-absenteeism-test:latest .
```

Run tests inside the docker container
```sh
docker-compose -f docker-compose.test.yml run --rm test
```
