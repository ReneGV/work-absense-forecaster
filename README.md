work-absense-forecaster
==============================

A work absense hours ML project

Setup

```
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt
```

Run MLFlow server
```
mlflow server  --backend-store-uri sqlite:///my.db   --default-artifact-root ./mlruns   --host 0.0.0.0 --port 5000
```


Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>

# Testing Guide for Work Absenteeism Forecaster

## ðŸ“‹ Test Structure

A comprehensive test suite with both **unit tests** and **integration tests** has been created:

```
tests/
â”œâ”€â”€ __init__.py                    # Package initialization
â”œâ”€â”€ conftest.py                    # Shared pytest fixtures (root level)
â”‚
â”œâ”€â”€ unit/                          # Unit tests (55 tests)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_preprocessors.py      # Tests for custom transformers
â”‚   â”œâ”€â”€ test_train_model.py       # Tests for training pipeline
â”‚   â”œâ”€â”€ test_predict_model.py     # Tests for prediction pipeline
â”‚   â””â”€â”€ test_parameter_tuning.py  # Tests for hyperparameter tuning
â”‚
â””â”€â”€ integration/                   # Integration tests (21 tests)
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ conftest.py                # Integration-specific fixtures
    â””â”€â”€ test_pipeline_integration.py  # End-to-end pipeline tests

Additional files:
â”œâ”€â”€ pytest.ini                     # Pytest configuration
â”œâ”€â”€ Dockerfile.test                # Docker image for testing
â””â”€â”€ docker-compose.test.yml        # Docker compose for running tests
```

## Test Coverage

### Unit Tests (`tests/unit/`)

1. **Preprocessors** (`test_preprocessors.py`)
   - `DropColumnsTransformer`: Column dropping functionality
   - `IQRClippingTransformer`: Outlier handling using IQR method
   - `ToStringTransformer`: Type conversion to strings
   - Integration with sklearn pipelines

2. **Model Training** (`test_train_model.py`)
   - Data loading and preparation
   - Pipeline construction
   - Model creation (Logistic Regression, Random Forest, Neural Network)
   - Training and evaluation
   - Multiple model training
   - Model persistence

3. **Model Prediction** (`test_predict_model.py`)
   - Model loading
   - Making predictions on new data
   - Data handling in prediction pipeline

4. **Data Utilities** (`test_data_utils.py`)
   - CSV file loading
   - Column name normalization
   - Data shape validation
   - Data value preservation

5. **Model Evaluation** (`test_evaluation.py`)
   - Metrics calculation (accuracy, F1, recall, precision)
   - Classification reports
   - Confusion matrix creation

### Integration Tests (`tests/integration/`)

**End-to-End Pipeline** (`test_pipeline_integration.py`)

1. **Complete ML Workflow** (`test_realistic_ml_workflow`)
   - Data loading and preparation
   - Train/test split
   - Preprocessing pipeline creation
   - Model training
   - Model persistence (save/load)
   - Prediction on new data
   - Metrics evaluation
   - Confusion matrix generation
   - File artifact verification

---

## ðŸš€ Quick Start

### Build Docker Image

Build the docker image that contains the required environment to run the tests:

```sh
docker build -f Dockerfile.test -t work-absenteeism-test:latest .
```

### Run All Tests

Run all tests (unit + integration):

```sh
docker-compose -f docker-compose.test.yml run --rm test
```

### Run Specific Test Suites

Run only unit tests:

```sh
docker-compose -f docker-compose.test.yml run --rm test pytest tests/unit/ -v
```

Run only integration tests:

```sh
docker-compose -f docker-compose.test.yml run --rm test pytest tests/integration/ -v
```

### Run with Coverage

Run tests with coverage report:

```sh
docker-compose -f docker-compose.test.yml run --rm test-coverage
```

### Test Markers

Use pytest markers to run specific test categories:

```sh
# Run only unit tests
pytest -m unit

# Run only integration tests  
pytest -m integration

# Run only slow tests
pytest -m slow
```

## ðŸ“Š Test Statistics

- **Total Tests**: 76
- **Unit Tests**: 55
- **Integration Tests**: 21
- **Test Coverage**: ~85% of `src/models` module
